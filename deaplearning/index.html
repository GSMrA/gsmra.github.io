<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Foot IA 3D - Stratégique & Mémorisation</title>
    <style>
        body { margin: 0; overflow: hidden; font-family: 'Segoe UI', sans-serif; background: #111; }
        #overlay {
            position: absolute; top: 10px; left: 10px; color: white;
            background: rgba(0,0,0,0.85); padding: 15px; border-radius: 10px;
            width: 360px; user-select: none; border: 1px solid #333;
            box-shadow: 0 4px 15px rgba(0,0,0,0.5);
        }
        .stat-row { display: flex; justify-content: space-between; margin-bottom: 5px; font-size: 13px; }
        h2 { margin: 0 0 10px 0; font-size: 18px; color: #4CAF50; border-bottom: 1px solid #444; padding-bottom: 5px; }
        input[type=range] { width: 100%; accent-color: #4CAF50; cursor: pointer; margin: 10px 0; }
        .bar-container { width: 100%; background: #333; height: 8px; border-radius: 5px; margin-top: 5px; overflow: hidden;}
        .bar-fill { height: 100%; transition: width 0.2s; }
        .bar-blue { background: #2196F3; }
        .bar-red { background: #f44336; }
        #logs { 
            font-family: monospace; font-size: 11px; color: #aaa; margin-top: 10px; 
            height: 100px; overflow-y: auto; background: rgba(0,0,0,0.3); padding: 5px; border-radius: 4px;
        }
        .status-led { display: inline-block; width: 10px; height: 10px; border-radius: 50%; background: #555; margin-right: 5px; }
        .training { background: #ffeb3b; box-shadow: 0 0 5px #ffeb3b; }
        .memory-stat { font-size: 10px; color: #888; text-align: right; margin-top: -5px;}
    </style>
    
    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/",
                "cannon-es": "https://unpkg.com/cannon-es@0.20.0/dist/cannon-es.js",
                "@tensorflow/tfjs": "https://esm.sh/@tensorflow/tfjs"
            }
        }
    </script>
</head>
<body>
    <div id="overlay">
        <h2>IA Tactique (Replay Buffer) <span id="statusLed" class="status-led ready"></span></h2>
        
        <div class="stat-row">
            <span>Score (Bleu v Rouge)</span>
            <span id="score" style="font-weight:bold; color:#fff">0 - 0</span>
        </div>

        <div style="margin-top: 10px;">
            <label>Vitesse Simulation: <span id="speedVal">1x</span></label>
            <input type="range" id="speedSlider" min="1" max="20" step="1" value="1">
        </div>

        <div class="stat-row" style="margin-top:10px">
            <span>Explo Bleu</span>
            <span id="epsilonBlueVal">50%</span>
        </div>
        <div class="bar-container"><div id="epsilonBlueBar" class="bar-fill bar-blue" style="width: 50%;"></div></div>

        <div class="stat-row" style="margin-top:5px">
            <span>Explo Rouge</span>
            <span id="epsilonRedVal">50%</span>
        </div>
        <div class="bar-container"><div id="epsilonRedBar" class="bar-fill bar-red" style="width: 50%;"></div></div>
        
        <div class="memory-stat" id="memSize">Mémoire: 0 exp</div>
        <div id="logs">Initialisation tactique...</div>
    </div>

    <script type="module">
        import * as THREE from 'three';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
        import * as CANNON from 'cannon-es';
        import * as tf from '@tensorflow/tfjs';

        // --- CONFIGURATION ---
        const CONFIG = {
            batchSize: 32,          // Taille du lot d'apprentissage
            memorySize: 2000,       // Mémoire maximale
            discountFactor: 0.95,   // Gamma (importance du futur)
            learningRate: 0.002,    // Vitesse d'apprentissage
            epsilonStart: 0.5,      // Exploration initiale (50% stratégique, 50% exploit)
            epsilonMin: 0.05,       // Minimum 5%
            epsilonDecay: 0.9995
        };

        // --- LOGGER ---
        function log(msg) {
            const el = document.getElementById('logs');
            const time = new Date().toLocaleTimeString().split(' ')[0];
            el.innerHTML = `<div style="margin-bottom:2px"><span style="color:#666">[${time}]</span> ${msg}</div>` + el.innerHTML;
        }

        // --- SCENE THREE.JS ---
        const scene = new THREE.Scene();
        scene.background = new THREE.Color(0x1a1a1a);
        const camera = new THREE.PerspectiveCamera(50, window.innerWidth/window.innerHeight, 0.1, 1000);
        camera.position.set(45, 35, 0); 
        camera.lookAt(0, 0, 0);

        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.shadowMap.enabled = true;
        document.body.appendChild(renderer.domElement);

        const controls = new OrbitControls(camera, renderer.domElement);
        controls.enableDamping = true;

        const dl = new THREE.DirectionalLight(0xffffff, 1.5);
        dl.position.set(10, 30, 10);
        dl.castShadow = true;
        scene.add(dl);
        scene.add(new THREE.AmbientLight(0xffffff, 0.4));

        // --- PHYSIQUE ---
        const world = new CANNON.World({ gravity: new CANNON.Vec3(0, -9.81, 0) });
        const matGround = new CANNON.Material();
        const matPlayer = new CANNON.Material();
        const matBall = new CANNON.Material();

        world.addContactMaterial(new CANNON.ContactMaterial(matGround, matPlayer, { friction: 0.0, restitution: 0 }));
        world.addContactMaterial(new CANNON.ContactMaterial(matGround, matBall, { friction: 0.3, restitution: 0.7 }));
        world.addContactMaterial(new CANNON.ContactMaterial(matPlayer, matBall, { friction: 0.1, restitution: 0.5 }));

        const syncObjects = [];

        function createMeshBody(geo, mat, mass, pos, cMat, color) {
            const mesh = new THREE.Mesh(geo, new THREE.MeshStandardMaterial({ color: color }));
            mesh.castShadow = true; mesh.receiveShadow = true;
            scene.add(mesh);
            let shape = geo.type === 'BoxGeometry' 
                ? new CANNON.Box(new CANNON.Vec3(geo.parameters.width/2, geo.parameters.height/2, geo.parameters.depth/2))
                : new CANNON.Sphere(geo.parameters.radius);
            const body = new CANNON.Body({ mass: mass, material: cMat });
            body.addShape(shape);
            body.position.copy(pos);
            if (mass > 0 && geo.type !== 'SphereGeometry') body.fixedRotation = true;
            world.addBody(body);
            syncObjects.push({ mesh, body });
            return body;
        }

        // Terrain
        const groundBody = new CANNON.Body({ mass: 0, material: matGround });
        groundBody.addShape(new CANNON.Plane());
        groundBody.quaternion.setFromEuler(-Math.PI/2, 0, 0);
        world.addBody(groundBody);
        const groundMesh = new THREE.Mesh(new THREE.PlaneGeometry(30, 50), new THREE.MeshStandardMaterial({ color: 0x228b22 }));
        groundMesh.rotation.x = -Math.PI/2; groundMesh.receiveShadow = true;
        scene.add(groundMesh);

        function addWall(x, z, w, d) {
            const b = new CANNON.Body({ mass: 0 });
            b.addShape(new CANNON.Box(new CANNON.Vec3(w/2, 2, d/2)));
            b.position.set(x, 2, z);
            world.addBody(b);
        }
        addWall(16, 0, 2, 50); addWall(-16, 0, 2, 50);
        addWall(0, 26, 30, 2); addWall(0, -26, 30, 2);

        // Buts
        function createGoal(z, color) {
            const goalMat = new THREE.MeshStandardMaterial({color: color});
            const p1 = new THREE.Mesh(new THREE.BoxGeometry(0.5, 4, 0.5), goalMat); p1.position.set(4, 2, z); scene.add(p1);
            const p2 = new THREE.Mesh(new THREE.BoxGeometry(0.5, 4, 0.5), goalMat); p2.position.set(-4, 2, z); scene.add(p2);
            const p3 = new THREE.Mesh(new THREE.BoxGeometry(8.5, 0.5, 0.5), goalMat); p3.position.set(0, 4, z); scene.add(p3);
        }
        createGoal(-25, 0x0000ff); createGoal(25, 0xff0000);

        // Entités
        const blue = createMeshBody(new THREE.BoxGeometry(2,2,2), null, 5, new THREE.Vector3(0,1,15), matPlayer, 0x0088ff);
        const red = createMeshBody(new THREE.BoxGeometry(2,2,2), null, 5, new THREE.Vector3(0,1,-15), matPlayer, 0xff4444);
        const ball = createMeshBody(new THREE.SphereGeometry(0.8), null, 1, new THREE.Vector3(0,5,0), matBall, 0xffffff);
        ball.angularDamping = 0.5;

        // --- IA SYSTEM (REPLAY MEMORY) ---
        class ReplayBuffer {
            constructor(maxSize) {
                this.buffer = [];
                this.maxSize = maxSize;
            }
            add(experience) {
                if (this.buffer.length >= this.maxSize) this.buffer.shift();
                this.buffer.push(experience);
            }
            sample(batchSize) {
                const batch = [];
                for(let i=0; i<batchSize; i++) {
                    batch.push(this.buffer[Math.floor(Math.random() * this.buffer.length)]);
                }
                return batch;
            }
            size() { return this.buffer.length; }
        }

        const memoryBlue = new ReplayBuffer(CONFIG.memorySize);
        const memoryRed = new ReplayBuffer(CONFIG.memorySize);

        function createModel() {
            const model = tf.sequential();
            // Input 10: [MyX, MyZ, BallX, BallZ, BallVX, BallVZ, OppX, OppZ, DistMyGoal, DistOppGoal]
            model.add(tf.layers.dense({inputShape: [10], units: 64, activation: 'relu'}));
            model.add(tf.layers.dense({units: 64, activation: 'relu'}));
            model.add(tf.layers.dense({units: 32, activation: 'relu'}));
            model.add(tf.layers.dense({units: 2, activation: 'tanh'}));
            model.compile({ optimizer: tf.train.adam(CONFIG.learningRate), loss: 'meanSquaredError' });
            return model;
        }

        const modelBlue = createModel();
        const modelRed = createModel();

        let epsilonBlue = CONFIG.epsilonStart;
        let epsilonRed = CONFIG.epsilonStart;
        let scoreB = 0, scoreR = 0;

        // État plus complet pour la stratégie
        function getState(player, opponent, goalZ, ownGoalZ) {
            return [
                player.position.x/30, player.position.z/50,             // Moi
                ball.position.x/30, ball.position.z/50,                 // Balle
                ball.velocity.x/20, ball.velocity.z/20,                 // Vitesse Balle
                opponent.position.x/30, opponent.position.z/50,         // Adversaire
                (ownGoalZ - player.position.z)/50,                      // Dist Def
                (goalZ - player.position.z)/50                          // Dist Atk
            ];
        }

        // --- ENTRAINEMENT (MINI-BATCH) ---
        async function trainAgent(model, memory) {
            if (memory.size() < CONFIG.batchSize) return;

            const batch = memory.sample(CONFIG.batchSize);
            const states = batch.map(e => e.state);
            const nextStates = batch.map(e => e.nextState);
            
            // Prédiction par lot pour l'efficacité
            const tfStates = tf.tensor2d(states);
            const tfNextStates = tf.tensor2d(nextStates);
            
            const qNow = model.predict(tfStates);
            const qNext = model.predict(tfNextStates);
            
            const qNextData = await qNext.data(); // Récupérer les valeurs JS
            const qNowData = await qNow.data();
            
            // Calcul manuel du target Q-Value
            // Q(s,a) = r + gamma * max(Q(s', a'))
            // Comme on a output continu (tanh -1 à 1), on approxime le gradient
            
            const xInputs = [];
            const yTargets = [];

            for(let i=0; i<CONFIG.batchSize; i++) {
                const { state, action, reward } = batch[i];
                // Simulé: on veut que l'action prise se rapproche de (action_prise * reward_factor)
                // C'est une simplification pour l'espace continu
                
                // Si reward positif, on renforce l'action. Si négatif, on l'inverse.
                const currentQ = [qNowData[i*2], qNowData[i*2+1]];
                
                let targetX, targetZ;
                
                if (reward > 0) {
                     // On pousse vers l'action prise + un peu de la valeur future estimée
                     targetX = action[0] + (CONFIG.discountFactor * 0.1); 
                     targetZ = action[1] + (CONFIG.discountFactor * 0.1);
                } else {
                     // On s'éloigne de l'action prise
                     targetX = -action[0] * 0.5;
                     targetZ = -action[1] * 0.5;
                }
                
                // Clamping -1 à 1
                targetX = Math.max(-1, Math.min(1, targetX));
                targetZ = Math.max(-1, Math.min(1, targetZ));

                xInputs.push(state);
                yTargets.push([targetX, targetZ]);
            }
            
            await model.fit(tf.tensor2d(xInputs), tf.tensor2d(yTargets), { epochs: 1, verbose: 0 });
            
            tfStates.dispose(); tfNextStates.dispose();
            qNow.dispose(); qNext.dispose();
        }

        // --- LOGIQUE STRATEGIQUE ---
        function getStrategicExploration(player, goalZ, ownGoalZ) {
            // Au lieu de Random pur, on choisit une "intention"
            const rand = Math.random();
            const vec = new THREE.Vector3();
            
            if (rand < 0.4) {
                // 1. Interception (Aller où la balle va être)
                vec.copy(ball.position).addScaledVector(ball.velocity, 0.5).sub(player.position);
            } else if (rand < 0.7) {
                // 2. Positionnement Défensif (Entre balle et mon but)
                const midPoint = new THREE.Vector3().copy(ball.position).lerp(new THREE.Vector3(0, 0, ownGoalZ), 0.5);
                vec.copy(midPoint).sub(player.position);
            } else if (rand < 0.9) {
                // 3. Attaque (Frapper vers le but adverse)
                vec.set(0, 0, goalZ).sub(player.position); 
            } else {
                // 4. Chaos total
                vec.set(Math.random()-0.5, 0, Math.random()-0.5);
            }
            
            vec.normalize();
            return [vec.x, vec.z];
        }

        // --- GAME LOOP ---
        let simSpeed = 1;
        document.getElementById('speedSlider').addEventListener('input', (e) => {
            simSpeed = parseInt(e.target.value);
            document.getElementById('speedVal').innerText = simSpeed + "x";
        });

        function reset() {
            ball.position.set(0, 5, 0); ball.velocity.set(0,0,0); ball.angularVelocity.set(0,0,0);
            blue.position.set(0, 1, 15); blue.velocity.set(0,0,0);
            red.position.set(0, 1, -15); red.velocity.set(0,0,0);
        }

        let frameCount = 0;
        let lastStateBlue = null, lastActionBlue = null;
        let lastStateRed = null, lastActionRed = null;

        function update() {
            for(let i=0; i<simSpeed; i++) {
                world.step(1/60);
                frameCount++;

                // Décision IA toutes les 3 frames
                if (frameCount % 3 === 0) {
                    const sBlue = getState(blue, red, -25, 25);
                    const sRed = getState(red, blue, 25, -25);

                    // --- AGENT BLEU ---
                    let actBlue;
                    if (Math.random() < epsilonBlue) {
                        actBlue = getStrategicExploration(blue, -25, 25);
                    } else {
                        const p = modelBlue.predict(tf.tensor2d([sBlue]));
                        actBlue = Array.from(p.dataSync());
                        p.dispose();
                    }
                    blue.applyForce(new CANNON.Vec3(actBlue[0]*75, 0, actBlue[1]*75));

                    // --- AGENT ROUGE ---
                    let actRed;
                    if (Math.random() < epsilonRed) {
                        actRed = getStrategicExploration(red, 25, -25);
                    } else {
                        const p = modelRed.predict(tf.tensor2d([sRed]));
                        actRed = Array.from(p.dataSync());
                        p.dispose();
                    }
                    red.applyForce(new CANNON.Vec3(actRed[0]*75, 0, actRed[1]*75));

                    // --- SAUVEGARDE EN MEMOIRE (REPLAY) ---
                    // On récompense l'état PRÉCÉDENT basé sur la situation ACTUELLE
                    if (lastStateBlue) {
                        const dBall = blue.position.distanceTo(ball.position);
                        const dOwnGoal = ball.position.distanceTo(new THREE.Vector3(0,0,25)); // But bleu est à z=25 (défense) ou z=-25?
                        // Buts sont à +/- 25. Bleu commence à +15, attaque vers -25.
                        
                        let r = -0.01; // Coût de l'énergie (temps)
                        if (dBall < 1.5) r += 0.5; // Touche balle
                        if (ball.position.z < blue.position.z && ball.velocity.z < -0.5) r += 0.2; // Balle devant moi vers but
                        
                        // Récompense tactique : Être entre balle et son propre but (Défense)
                        const vecToBall = new THREE.Vector3().copy(ball.position).sub(blue.position);
                        const vecToOwnGoal = new THREE.Vector3(0,0,25).sub(blue.position); // But bleu à protéger est à Z=25? Non, rouge attaque Z=25.
                        // Vérif: Blue pos +15, Goal Z=-25. Red pos -15, Goal Z=25.
                        // Donc Blue protège Z=25. Red protège Z=-25.
                        
                        // Si je suis entre balle et but (Z=25)
                        if (blue.position.z > ball.position.z && blue.position.z < 25 && Math.abs(blue.position.x - ball.position.x) < 5) {
                            r += 0.05; // Bon placement défensif
                        }

                        memoryBlue.add({ state: lastStateBlue, action: lastActionBlue, reward: r, nextState: sBlue });
                    }
                    lastStateBlue = sBlue; lastActionBlue = actBlue;

                    if (lastStateRed) {
                        const dBall = red.position.distanceTo(ball.position);
                        let r = -0.01;
                        if (dBall < 1.5) r += 0.5;
                        if (ball.position.z > red.position.z && ball.velocity.z > 0.5) r += 0.2; // Pousse vers but
                        
                        // Défense rouge (Protège Z=-25)
                        if (red.position.z < ball.position.z && red.position.z > -25 && Math.abs(red.position.x - ball.position.x) < 5) {
                            r += 0.05;
                        }
                        
                        memoryRed.add({ state: lastStateRed, action: lastActionRed, reward: r, nextState: sRed });
                    }
                    lastStateRed = sRed; lastActionRed = actRed;

                    // Décroissance Epsilon
                    if (epsilonBlue > CONFIG.epsilonMin) epsilonBlue *= CONFIG.epsilonDecay;
                    if (epsilonRed > CONFIG.epsilonMin) epsilonRed *= CONFIG.epsilonDecay;
                }

                // --- BUTS ---
                if (ball.position.z < -26) { // But Bleu
                    scoreB++;
                    log("BUT BLEU! Récompense différée.");
                    document.getElementById('score').innerText = `${scoreB} - ${scoreR}`;
                    // On ajoute une grosse expérience finale
                    if(lastStateBlue) memoryBlue.add({state: lastStateBlue, action: lastActionBlue, reward: 5, nextState: lastStateBlue});
                    if(lastStateRed) memoryRed.add({state: lastStateRed, action: lastActionRed, reward: -3, nextState: lastStateRed});
                    
                    reset();
                    lastStateBlue=null; lastStateRed=null;
                }
                else if (ball.position.z > 26) { // But Rouge
                    scoreR++;
                    log("BUT ROUGE! Récompense différée.");
                    document.getElementById('score').innerText = `${scoreB} - ${scoreR}`;
                    if(lastStateRed) memoryRed.add({state: lastStateRed, action: lastActionRed, reward: 5, nextState: lastStateRed});
                    if(lastStateBlue) memoryBlue.add({state: lastStateBlue, action: lastActionBlue, reward: -3, nextState: lastStateBlue});
                    
                    reset();
                    lastStateBlue=null; lastStateRed=null;
                }
            }

            // Entraînement asynchrone (1 fois par frame visuelle pour ne pas laguer la physique)
            trainAgent(modelBlue, memoryBlue);
            trainAgent(modelRed, memoryRed);
        }

        function animate() {
            requestAnimationFrame(animate);
            update();
            
            syncObjects.forEach(o => {
                o.mesh.position.copy(o.body.position);
                o.mesh.quaternion.copy(o.body.quaternion);
            });

            // UI Updates
            document.getElementById('epsilonBlueBar').style.width = (epsilonBlue*100) + "%";
            document.getElementById('epsilonBlueVal').innerText = Math.floor(epsilonBlue*100) + "%";
            document.getElementById('epsilonRedBar').style.width = (epsilonRed*100) + "%";
            document.getElementById('epsilonRedVal').innerText = Math.floor(epsilonRed*100) + "%";
            document.getElementById('memSize').innerText = `Mémoire: ${memoryBlue.size()} exp`;

            controls.update();
            renderer.render(scene, camera);
        }
        
        reset();
        animate();
    </script>
</body>
</html>
